<pre>
import sys, os, re,datetime, glob
import time as t
import subprocess
from cStringIO import StringIO
import ftplib, zipfile
import numpy as np

from netCDF4 import Dataset
import h5py


#Settings
import settings

'''
Converts  LOCA netCDF files to hdf5 format.
data for each element are stored in
netCDF files.
Each hdf5 output file will contain data for one year for all elements.
Uses h5py, netCDF4 python modules
on cyclone1:
these modules are installed in special virtual env scipytest
alias net='source /usr/local/pythonenv/scipytest/bin/activate'
on bitz1:
these modules are installed system wide
Created By: Britta Daudert
'''

class NetCDFToHDF5Converter(object):
    def __init__(self, year,model,rcp,base_dir,h5_name, version,data_type,log) :
        self.year = year #hdf5 file year
        self.base_dir = base_dir #directory in which hdf5 files will be stored
        self.dirty = False
        self.h5_name = h5_name #year.h5
        self.model_name = model #model name
        self.rcp = rcp
        self.version = version #dataset version (1)
        self.data_type = data_type #maca,loca,maurer,...
        self.log = log
        self.grid_def = getattr(settings,'grid_def_' + self.data_type )
        self.element_list = self.grid_def['elems'].keys() #List of climate elements

    def is_leap_year(self, year):
        '''
        Check if year is leap year.
        '''
        yr = int(year)
        if yr % 100 != 0 and yr % 4 == 0:
            return True
        elif yr % 100 == 0 and yr % 400 == 0:
            return True
        else:
            return False

    def open_grid(self) :
        '''
        Opens hdf5 file object for appending
        '''
        self.h5 = h5py.File(self.base_dir + self.h5_name,'a')

    def close(self) :
        '''
        Closes hdf5 file object
        '''
        self.h5.close()


    def create_grid(self) :
        '''
        Initializes hdf5 data structure
        dimensions: (element, time, lat,lon)
        Each hdf5 file holds data for one year for each element
        at each lat/lon
        '''
        self.h5 = h5 = h5py.File(self.base_dir + self.h5_name,'w-')
        shape_v, shape_h = self.grid_shape
        for elem in self.element_list:
            fV= self.grid_def['elems'][elem]['fillValue']
            e = h5.create_dataset(elem,(1,shape_v,shape_h),dtype='i2',fillvalue=fV,
                compression='lzf',chunks=(1,shape_v//4,shape_h//4),
                maxshape=(366,shape_v,shape_h))
            e.attrs.create('units',self.grid_def['elems'][elem]['units'])


    def set_loca_file_name(self,year, var_name_short, rcp):
        f_name = var_name_short + '_day_' + self.model_name
        if int(year) <= 2005:
            if self.model_name == 'CCSM4':
                f_name+='_historical_r6i1p1_'
            else:
                f_name+='_historical_r1i1p1_'
        else:
            if self.model_name == 'CCSM4':
                f_name+= '_' +  rcp + '_r6i1p1_'
            else:
                f_name+= '_' +  rcp + '_r1i1p1_'
        f_name+= str(year) + '0101' + '-' +  str(year) + '1231'
        #f_name+='.LOCA_2015-02-20.1-16deg.nc'
        f_name+= '.LOCA_2016-04-02.16th.nc'
        return f_name

    def download_data(self,file_name):
        '''
        Download dataset file
        '''
        def printTotals(transferred, toBeTransferred):
            print "Transferred: {0}\tOut of: {1}".format(transferred, toBeTransferred)
        base_path = settings.LOCA_REMOTE_PATH
        items = file_name.split('.')[0].split('_')
        print file_name
        log.write('Downloading file: ' + str(file_name))
        var = items[0]
        model = items[2]
        period = items[3]
        if model == 'CCSM4':
            base_path+=model + '/16th/' + period + '/r6i1p1/' + var + '/'
        else:
            base_path+=model + '/16th/' + period + '/r1i1p1/' + var + '/'

        print base_path
        import paramiko
        count = 1
        while count <= 5:
            try:
                transport = paramiko.Transport((settings.LOCA_SERVER, settings.LOCA_PORT))
                transport.connect(username = settings.LOCA_USER, password = settings.LOCA_PW)
                sftp = paramiko.SFTPClient.from_transport(transport)
                sftp.get(base_path + file_name,'./' + file_name,callback=printTotals)
                sftp.close()
                transport.close()
                break
            except Exception, e:
                if count < 5:
                    print 'Error retieving data. Trying again!'
                    print 'Error: %s' %str(e)
                if count == 5:
                    log.write('Error retieving data from  %s\n Error: %s'%(base_path + file_name,str(e)))


    def get_elem_array(self,net_file,el_name_short):
        '''
        Given the netCDF file and the element name,
        we download the according netCDF file amd
        get the element data and the time stamp
        '''
        el_name_long = self.grid_def['elems'][el_name_short]['var_name_long']
        if not os.path.isfile(net_file):
            print 'Downloading  %s' %net_file
            self.download_data(net_file)
            if isinstance(type(net_file),str):
                log.write('Could not download data file: %s' %net_file)
                sys.exit(1)
        if not os.path.isfile(net_file):
            log.write('Could not download data file: %s' %net_file)
            sys.exit(1)
        f = filter(os.path.isfile, glob.glob(net_file))
        if len(f) != 1:log.write('Can not find file: %s in %s' %(net_file, str(os.getcwd())))
        self.dataset = Dataset(f[0], 'r')
        try:
            elem_data = self.dataset.variables[el_name_long]
        except:
            elem_data = self.dataset.variables[el_name_short]
        self.grid_shape = (len(self.dataset.variables['lat']), len(self.dataset.variables['lon']))
        try:
            creation_date = str(self.dataset.getncattr('creation_date'))
        except:
            creation_date = str(self.dataset.getncattr('input1_creation_date'))
        date_t = datetime.datetime.strptime(creation_date, '%Y-%m-%dT%H:%M:%SZ')
        time_stamp = t.mktime(date_t.timetuple())
        if os.path.isfile(net_file):
            os.remove(net_file)
            #pass
        return elem_data,time_stamp

    def save_array(self, h5_elem,doy_idx,data, time_stamp):
        '''
        Saves data array to hdf5 file
        '''
        if h5_elem.shape[0] <= doy_idx :
            h5_elem.resize((doy_idx+1,)+self.grid_shape)
        h5_elem[doy_idx,...] = data

    def convert_data(self,time_data, elem):
        '''
        Since data in hdf5 file are stored as integers,
        we need to conver the element data from the netCDF files to integer values.
        '''
        fract = self.grid_def['elems'][elem]['units'].split(' ')[0]
        fact = int(1/float(fract))
        new_data = time_data
        np_data = np.array(time_data)
        idx_array = np.where(np_data >-9998.0)
        idx_array_fill = np.where(np_data < -9998.0)
        sub = 0
        if elem in ['tasmax','tasmin']:
            sub = 273 #Convert Kelvin to C
        elif elem ==  'pr':
            fact = fact * 86400 #Convert kg/m^2s to mm/day (kg/m^2s ~ mm/s)
            #http://onlineconversion.vbulletin.net/forum/main-forums/convert-and-calculate/13346-convert-total-precipitation-kg-m-2-to-rainfall-mm-day
        elif elem == 'huss':
            fact=fact*1000  #convert from kg/kg to g/kg
        np_data[idx_array] = np.floor(fact*(np_data[idx_array] - sub)).astype(int)
        np_data[idx_array_fill] = self.grid_def['elems'][elem]['fillValue']
        new_data = np_data.tolist()
        return new_data


    def load_dly(self):
        '''
        Loads element data from netCDF into hdf5 file
        '''
        for elem in self.grid_def['elems'].keys():
            print 'Loading element %s' %elem
            el_name_long = self.grid_def['elems'][elem]['var_name_long']
            net_file = self.set_loca_file_name(self.year, elem, self.rcp)
            elem_data,time_stamp = self.get_elem_array(net_file, elem)
            log.write('netCDF file:   %s\n' %net_file)
            log.write('Converting  %s\n' %elem)
            if os.path.isfile(self.base_dir + self.h5_name) :
                self.open_grid()
            else :
                self.create_grid()

            for doy_idx in range(len(elem_data)):
                #Convert data entires to integers
                scaled_data = self.convert_data(elem_data[doy_idx],elem)
                '''
                #Note: ACIS non-leap years have 365 days, but LOCA data arrays are always 366
                if not self.is_leap_year(self.year) and doy_idx >= 60:
                    doy = doy_idx - 1
                else:
                    doy - doy_idx
                '''
                self.save_array(self.h5[elem],doy_idx,scaled_data, time_stamp)
            log.write('Saved data arrays \n')
        self.dataset.close()
        self.close()


########
#M A I N
########
if __name__ == '__main__' :
    data_type = 'loca'
    rcps = ['rcp45','rcp85']
    models= settings.LOCA_CMIP5_MODELS.keys()
    version = 'v2'
    local_dir = settings.LOCA_LOCAL_DIR
    #for model in models:
    for model in models:
        base_dir = local_dir + model + '/'
        log = open(data_type + '_' + model + '_load.log','a+')
        log.write(model)
        log.write('*** start %s ***\n'%(t.ctime()))
        for rcp in rcps:
            data_dir = base_dir + rcp + '/'
            #Checks that local_dir exists and is writable
            try:
                if not os.path.exists(data_dir):
                    os.makedirs(data_dir)
            except Exception, e:
                log.write('ERROR creating local dir: ' + str(e))
                sys.exit(1)
            #For testing only do one file
            #for year in range(1950,2100):
            for year in range(1950,2100):
                log.write('PROCESSING YEAR: %s\n' %(str(year)))
                print 'Converting year %s' %str(year)
                h5_name = '%s.h5' %(str(year))
                #Remove old files
                if os.path.isfile(data_dir + h5_name):
                    os.remove(data_dir + h5_name)
                FC =  NetCDFToHDF5Converter(year,model,rcp,data_dir,h5_name,version,data_type,log)
                #Make sure basedir exists on local host
                FC.load_dly()
                '''
                try:
                    FC.load_dly()
                    log.write('SUCCESS!')
                except Exception, e :
                    print 'FAILED with ERROR: %s' %str(e)
                    log.write('FAIL!')
                    log.write(str(e))
                '''
</pre>
